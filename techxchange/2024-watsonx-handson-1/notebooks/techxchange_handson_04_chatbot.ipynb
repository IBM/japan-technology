{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"30f97e0a-bdbf-49f2-b7ae-dff2126b2fdf"},"source":["# TechXchange Japan 2024: さわってみようベクトル・データベース watsonx.dataでRAG体験\n","\n","\n","生成AIの回答精度を向上させるために、自社内のデータを活用してみましょう！<br>\n","ベクトル・データベース + 大規模言語モデル（LLM）で構成されるRAGのアーキテクチャーを使えば、自社内の情報で生成AIチャットボットが作成できます。<br>\n","当ハンズオンでは「TechXchange Japan 2024」に関するデータを使ってRAGを構成し、「TechXchange Japan 2024」に関することを教えてくれるチャットボットを作成します。<br>\n","(以下ベクトル・データベースはベクトルDBと表記します。)<br>\n","\n","\n","具体的には大規模言語モデル（LLM）を使用したアプリケーション開発のためのオープンソース・オーケストレーション・フレームワーク[LangChain](https://python.langchain.com/docs/introduction/)を使って、wastosonx.dataのベクトルDB **Milvus**に「TechXchange Japan 2024」に関するデータをロードし、watson.aiで提供されているLLMを使用してRAGを構成し、「TechXchange Japan 2024」のことを回答してくれるChatbotを作ってみます。\n","\n","ハンズオンは以下の順序で実行します:\n","\n","1. Excelをベクトル化してベクトルDB Milvusに入れよう！\n","2. ベクトルDB Milvusに入ったデータで類似検索してみよう!\n","3. ベクトルDB Milvusとwatsonx.ai LLMでRAGを構成して、質問をしてみよう!\n","4. **ベクトルDB Milvusとwatsonx.ai LLMでRAGを構成して、チャットアプリを作成してみよう!**[**当notebook**]\n","\n","\n","このハンズオンのガイドは[**こちら**](https://ibm.biz/20241127-rag-handson)にあります。"]},{"cell_type":"markdown","metadata":{"id":"7b068503-dc6e-49bb-8ab5-4a538a127dbd"},"source":["## 4. ベクトルDB Milvusとwatsonx.ai LLMでRAGを構成して、チャットアプリを作成してみよう!\n","\n","**実行は　セルを選択(クリック)して**\n","\n","- **Windows： Ctrl+Enter**\n","- **Mac: ⌘ (command) + Enter 　または Ctrl + Enter**\n","\n","セルの左側に[*]が表示されている場合は実行中です。<br>\n","[1]のように数字が入っている場合は、実行が完了しています。"]},{"cell_type":"markdown","metadata":{"id":"b35cf2d1-5d74-4357-bb8e-d35d094cb2eb"},"source":["### 1. 必要なライブラリーのインストール"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"73118d28-e88b-4974-bd30-afe87a57e9d6"},"outputs":[],"source":["!pip install -U 'ibm-watsonx-ai>=1.1.15'\n","!pip install -Uq 'langchain>=0.3.3'\n","!pip install -Uq 'langchain-ibm>=0.3.1'\n","!pip install -Uq 'langchain-huggingface>=0.1.0'\n","!pip install -Uq 'langchain-milvus>=0.1.6'\n","!pip install -Uq 'langchain-community>=0.3.2'\n","!pip install -Uq 'pymilvus>=2.4.8'\n","!pip install -U gradio"]},{"cell_type":"markdown","metadata":{"id":"7182c48f-86a8-4cfd-934e-911c99778b0b"},"source":["**インストール終了後、一旦カーネルを再起動してください** <br>\n","\n","**手順:**\n","- 上部のメニュー「Karnel」から「Restart Karnel and Clear Outputs of All Cells...」をクリック\n","- 「Restart Kernel?」 のダイアログが表示されるので、「Restart」をクリック"]},{"cell_type":"markdown","metadata":{"id":"f7ea1850-535b-424b-bf95-7c9a30dd56c6"},"source":["### 2. apikeyの設定 \n","\n","- 実行するとテキスト入力ボックスが表示されるので、事前に取得したapikeyをに入れてEnter Keyを押してください\n","\n","- ハンズオン環境で実行の場合は、`IBM Cloud Service API key`を入力してください。　よくわからない場合は[こちら](https://github.com/IBM/japan-technology/tree/main/techxchange/2024-watsonx-handson-1/01_techzone_use_environments.md)の2-3を参照してください。\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f18f4155-7339-4fe2-a2dd-e7420b237e89"},"outputs":[],"source":["import getpass\n","apikey = getpass.getpass(\"apikeyを入れてEnter Keyを押してください:\")"]},{"cell_type":"markdown","metadata":{"id":"c9621738-d9da-4903-9748-46ce74b61a02"},"source":["### 3. Milvus接続情報の設定\n","\n","実行すると順に以下の2つのテキスト入力ボックスが表示されるので、事前に取得したMilvus接続情報の値を入力してください。\n","- milvus GRPC ホストを入れてEnter Keyを押してください →  GRPC ホストの値を入力  \n","\n","- milvus GRPC ポートを入れてEnter Keyを押してください　→ GRPC ポートの値をを入力\n","\n","\n","Milvus接続情報の詳細取得手順は[こちら](https://github.com/IBM/japan-technology/tree/main/techxchange/2024-watsonx-handson-1/watsonx_data_get_milvus_info.md) を参照\n","\n","簡略な接続手順は以下です:\n","- watsonx.dataの画面を開く\n","- ナビゲーションメニューから「インフラストラクチャー・マネージャー」を選択\n","- サービス「Milvus」をクリック\n","- タイプの下の「接続の詳細を見る」をクリック\n","- GRPC ホストの値とGRPC ポートの値を取得\n","  \n","collection名は`techxchange_line_data`としています。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a35e9257-f805-4eed-961e-549e439cbed6"},"outputs":[],"source":["milvus_host=input(\"milvus GRPC ホストを入れてEnter Keyを押してください: \")\n","milvus_port=input(\"milvus GRPC ポートを入れてEnter Keyを押してください: \")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4b4db79f-0921-4da1-a44b-79c7a9e5bc0f"},"outputs":[],"source":["# Milvus接続情報パラメータののセット\n","my_connection_args ={\n"," 'uri': f'https://{milvus_host}:{milvus_port}', \n"," 'token': f'ibmlhapikey:{apikey}'\n","}\n","my_collection =  'techxchange_line_data'"]},{"cell_type":"markdown","metadata":{"id":"45a22cc1-58cb-4ce5-aee9-7518c9c8b803"},"source":["### 3. watsonx.ai Project idの設定\n","\n","Watson Studioで実行する場合は、このノートブックが実行されるプロジェクトからProject idを取得します。\n","Watson Studio以外で実行する場合は、Project idを入力してください。\n","\n","**Hint**: `project_id` はプロジェクトを表示し、管理タブから `project_id` を取得可能です."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"db063c74-f624-475b-b351-11cc27935bae"},"outputs":[],"source":["import os\n","try:\n","    project_id = os.environ[\"PROJECT_ID\"]\n","except KeyError:\n","    project_id = input(\"Project idを入力してください (入力後enter): \")"]},{"cell_type":"markdown","metadata":{"id":"754832b9-f4bd-4df0-ab5a-4992aa70bae1"},"source":["### 5.watsonx.aiのAuthentication用のエンドポイントのURLの設定\n","\n","Waston Machine Learningのインスタンスを作成したリージョンで決まります。\n","https://ibm.github.io/watson-machine-learning-sdk/setup_cloud.html#authentication　より\n","\n","- Dallas: https://us-south.ml.cloud.ibm.com\n","- London: https://eu-gb.ml.cloud.ibm.com\n","- Frankfurt: https://eu-de.ml.cloud.ibm.com\n","- Tokyo: https://jp-tok.ml.cloud.ibm.com\n","\n","今回はダラスのWaston Machine Learningのインスタンスを使っているので`https://us-south.ml.cloud.ibm.com`を使います。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cfd52f14-a187-4a1d-9f40-abb4437d02a2"},"outputs":[],"source":["watsonx_url = \"https://us-south.ml.cloud.ibm.com\" #watsonx.aiのAuthentication用のエンドポイントのURL"]},{"cell_type":"markdown","metadata":{"id":"3dd48de7-ae9a-420a-9de3-9ba33c1f429b"},"source":["### 4. 必要ライブラリーのImport"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5a2b3fae-312a-4bb9-9314-1a1a37dea809"},"outputs":[],"source":["import pandas as pd\n","from langchain.schema.document import Document\n","import json\n","from langchain_huggingface import HuggingFaceEmbeddings\n","from langchain_milvus import Milvus\n","import os\n","from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n","from langchain_ibm import WatsonxLLM\n","from langchain.prompts import PromptTemplate\n","from langchain.schema.runnable import RunnablePassthrough\n","\n","\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""]},{"cell_type":"markdown","metadata":{"id":"848a8cdb-cb32-482a-9aed-0a3c0f6d5f1d"},"source":["### 4. Embeddingモデルの取得\n","ベクトル化した時と同じモデル`intfloat/multilingual-e5-large`を使います<br>\n","https://huggingface.co/intfloat/multilingual-e5-large\n","\n","`TqdmExperimentalWarning`のWarningが表示された場合は無視でよいです。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d0cf8eef-ec2a-4195-99e6-c23140e47d2b"},"outputs":[],"source":["embeddings = HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-large\")"]},{"cell_type":"markdown","metadata":{"id":"9a643608-e108-4932-8e94-4a0631b874ac"},"source":["### 5.  ベクトルDB Milvusに接続"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3655cc28-7d24-4437-9e1c-9de6b30392fc"},"outputs":[],"source":["from langchain_milvus import Milvus\n","\n","vector_db = Milvus(\n","    embeddings,\n","    connection_args =my_connection_args,\n","    collection_name = my_collection\n",")"]},{"cell_type":"markdown","metadata":{"id":"906f6ad1-265d-4ab5-a613-143af4093394"},"source":["### 6.  watsonx.ai LLMの取得　および　プロンプトテンプレートの作成\n","\n","今回3つ切り替えて試せるようにします。以下の3つを取得し、それに適したプロンプトテンプレートを作成します。\n","\n","- mixtral-8x7b-instruct-v01\n","- llama-3-2-3b-instruct\n","- granite-3-8b-instruct"]},{"cell_type":"markdown","metadata":{"id":"ef71f36a-6e77-4b77-9b92-5093bb416379"},"source":["#### mixtral-8x7b-instruct-v01"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"10cbd80d-950e-40df-aafa-9d83c8023cc6"},"outputs":[],"source":["custom_llm = {}\n","rag_prompt = {}\n","no_rag_prompt = {}\n","\n","# 使用するLLMのパラメータ: mixtral-8x7b-instruct-v0\n","key = 'mixtral-8x7b-instruct-v01'\n","generate_params = {\n","    GenParams.MAX_NEW_TOKENS: 16384,\n","    GenParams.MIN_NEW_TOKENS: 0,\n","    GenParams.DECODING_METHOD: \"greedy\",\n","    GenParams.REPETITION_PENALTY: 1\n","}\n","\n","# LangChainで使うllm: mixtral-8x7b-instruct-v0\n","custom_llm[key] = WatsonxLLM(\n","    model_id=\"mistralai/mixtral-8x7b-instruct-v01\",\n","    url=watsonx_url,\n","    apikey=apikey,\n","    project_id=project_id,\n","    params=generate_params,\n",")\n","\n","# LangChainで使うPrompt(RAGあり): mixtral-8x7b-instruct-v0\n","template = \"\"\"<s> [INST] \n","あなたは親切で、礼儀正しく、誠実なアシスタントです。常に安全を保ちながら、できるだけユーザーの役に立つように詳しく回答してください。\n","回答には、有害、非倫理的、⼈種差別的、性差別的、有毒、危険、または違法なコンテンツを含めてはいけません。回答は社会的に偏⾒がなく、本質的に前向きなものであることを確認してください。\n","質問が意味をなさない場合、または事実に⼀貫性がない場合は、正しくないことに答えるのではなく、その理由を説明してください。質問の答えがわからない場合は、誤った情報を共有しないでください。\n","questionに答えるために、以下のcontextを使用し必ず日本語でanswerを作成してください。\n","必ず⽇本語の文章で回答してください。知ったかぶりをしないでください。\n","回答を書くときは、context内の単語をできるだけ使⽤してください。context中に質問に対する回答が無い低い場合は、「文書中に質問に対する回答が明記されていません。」とだけ回答してください。「文書中に質問に対する回答が明記されていません。」と回答した場合、そこで回答を終わりにしてください。\n","contextの内容がブランクの場合、「文書中に質問に対する回答が明記されていません。」とだけ回答してください。\n","セッションについて回答する場合は、タイトル、開始時刻、終了時刻、会場、概要、レベルを回答してください。回答が200文字以上の場合、回答はなるべく箇条書きを含めてわかりやすく回答してください。\n","[/INST]\n","</s>\n","<s> [INST] 質問が質問の文章ではなく意味がわからない場合[/INST]もう少し詳しく説明していただけますか？</s>\n","\n","context: {context}\n","question: {question}\n","answer: \n","\"\"\"\n","\n","# Prompt Templateを作成します\n","rag_prompt[key] = PromptTemplate.from_template(template)\n","\n","\n","# LangChainで使うPrompt(RAGなし): mixtral-8x7b-instruct-v0\n","template = \"\"\"<s> [INST] \n","必ず⽇本語の文章で回答してください。知ったかぶりをしないでください。。\n","あなたは親切で、礼儀正しく、誠実なアシスタントです。常に安全を保ちながら、できるだけユーザーの役に立つように詳しく回答してください。\n","回答には、有害、非倫理的、⼈種差別的、性差別的、有毒、危険、または違法なコンテンツを含めてはいけません。回答は社会的に偏⾒がなく、本質的に前向きなものであることを確認してください。\n","質問が意味をなさない場合、または事実に⼀貫性がない場合は、正しくないことに答えるのではなく、その理由を説明してください。質問の答えがわからない場合は、誤った情報を共有しないでください。\n","[/INST]\n","</s>\n","<s> [INST] 質問が質問の文章ではなく意味がわからない場合[/INST]もう少し詳しく説明していただけますか？</s>\n","question: {question}\n","answer: \n","\"\"\"\n","\n","# Prompt Templateを作成します\n","no_rag_prompt[key] = PromptTemplate.from_template(template)"]},{"cell_type":"markdown","metadata":{"id":"7979b79c-f867-422e-9578-eaf2ed73fd94"},"source":["#### llama-3-2-3b-instruct"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"24e57125-7b63-4b99-94d5-88762a304eff"},"outputs":[],"source":["# 使用するLLMのパラメータ:llama-3-2-3b-instruct\n","key = 'llama-3-2-3b-instruct'\n","generate_params = {\n","    GenParams.MAX_NEW_TOKENS: 8192,\n","    GenParams.MIN_NEW_TOKENS: 0,\n","    GenParams.DECODING_METHOD: \"greedy\",\n","    GenParams.REPETITION_PENALTY: 1\n","}\n","\n","# LangChainで使うllm: llama-3-2-3b-instruct\n","custom_llm[key] = WatsonxLLM(\n","    model_id=\"meta-llama/llama-3-2-3b-instruct\",\n","    url=watsonx_url,\n","    apikey=apikey,\n","    project_id=project_id,\n","    params=generate_params,\n",")\n","\n","# LangChainで使うPrompt(RAGあり): llama-3-2-3b-instruct\n","template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n","あなたは親切で、礼儀正しく、誠実なアシスタントです。常に安全を保ちながら、できるだけユーザーの役に立つように詳しく回答してください。\n","回答には、有害、非倫理的、⼈種差別的、性差別的、有毒、危険、または違法なコンテンツを含めてはいけません。回答は社会的に偏⾒がなく、本質的に前向きなものであることを確認してください。\n","質問が意味をなさない場合、または事実に⼀貫性がない場合は、正しくないことに答えるのではなく、その理由を説明してください。質問の答えがわからない場合は、誤った情報を共有しないでください。\n","questionに答えるために、以下のcontextを使用し必ず日本語でanswerを作成してください。\n","必ず⽇本語の文章で回答してください。知ったかぶりをしないでください。\n","回答を書くときは、context内の単語をできるだけ使⽤してください。context中に質問に対する回答が無い低い場合は、「文書中に質問に対する回答が明記されていません。」とだけ回答してください。「文書中に質問に対する回答が明記されていません。」と回答した場合、そこで回答を終わりにしてください。\n","contextの内容がブランクの場合、「文書中に質問に対する回答が明記されていません。」とだけ回答してください。\n","セッションについて回答する場合は、タイトル、開始時刻、終了時刻、会場、概要、レベルを回答してください。回答が200文字以上の場合、回答はなるべく箇条書きを含めてわかりやすく回答してください。\n","<|eot_id|><|start_header_id|>user<|end_header_id|>\n","\n","context: {context} <|eot_id|><|start_header_id|>user<|end_header_id|>\n","\n","{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n","\"\"\"\n","\n","# Prompt Templateを作成します\n","rag_prompt[key] = PromptTemplate.from_template(template)\n","\n","# LangChainで使うPrompt(RAGなし): llama-3-2-3b-instruct\n","template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n","あなたは親切で、礼儀正しく、誠実なアシスタントです。常に安全を保ちながら、できるだけユーザーの役に立つように詳しく回答してください。\n","回答には、有害、非倫理的、⼈種差別的、性差別的、有毒、危険、または違法なコンテンツを含めてはいけません。回答は社会的に偏⾒がなく、本質的に前向きなものであることを確認してください。\n","必ず⽇本語の文章で回答してください。知ったかぶりをしないでください。\n","質問が意味をなさない場合、または事実に⼀貫性がない場合は、正しくないことに答えるのではなく、その理由を説明してください。質問の答えがわからない場合は、誤った情報を共有しないでください。<|eot_id|><|start_header_id|>user<|end_header_id|>\n","\n","{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n","\"\"\"\n","\n","# Prompt Templateを作成します\n","no_rag_prompt[key] = PromptTemplate.from_template(template)"]},{"cell_type":"markdown","metadata":{"id":"28cfb9f4-ec2d-4a0b-817d-1d1ce8198550"},"source":["####  granite-3-8b-instruct"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c2b221ca-152d-45de-85c5-74a5980acc91"},"outputs":[],"source":["# 使用するLLMのパラメータ: granite-3-8b-instruct\n","key = 'granite-3-8b-instruct'\n","generate_params = {\n","    GenParams.MAX_NEW_TOKENS: 4096,\n","    GenParams.MIN_NEW_TOKENS: 0,\n","    GenParams.DECODING_METHOD: \"greedy\",\n","    GenParams.REPETITION_PENALTY: 1\n","}\n","\n","# LangChainで使うllm: granite-3-8b-instruct\n","custom_llm[key] = WatsonxLLM(\n","    model_id=\"ibm/granite-3-8b-instruct\",\n","    url=watsonx_url,\n","    apikey=apikey,\n","    project_id=project_id,\n","    params=generate_params,\n",")\n","\n","# LangChainで使うPromptt(RAGあり): granite-3-8b-instruct\n","template = \"\"\"<|start_of_role|>system<|end_of_role|>なたは2024年にIBMが開発したAI言語モデル「Granite」です。あなたは親切で、礼儀正しく、誠実なアシスタントです。常に安全を保ちながら、できるだけユーザーの役に立つように詳しく回答してください。\n","回答には、有害、非倫理的、⼈種差別的、性差別的、有毒、危険、または違法なコンテンツを含めてはいけません。回答は社会的に偏⾒がなく、本質的に前向きなものであることを確認してください。\n","質問が意味をなさない場合、または事実に⼀貫性がない場合は、正しくないことに答えるのではなく、その理由を説明してください。質問の答えがわからない場合は、誤った情報を共有しないでください。\n","questionに答えるために、以下のcontextを使用し必ず日本語でanswerを作成してください\n","必ず⽇本語の文章で回答してください。知ったかぶりをしないでください。\n","回答を書くときは、context内の単語をできるだけ使⽤してください。context中に質問に対する回答が無い低い場合は、「文書中に質問に対する回答が明記されていません。」とだけ回答してください。「文書中に質問に対する回答が明記されていません。」と回答した場合、そこで回答を終わりにしてください。\n","contextの内容がブランクの場合、「文書中に質問に対する回答が明記されていません。」とだけ回答してください。\n","セッションについて回答する場合は、タイトル、開始時刻、終了時刻、会場、概要、レベルを回答してください。回答が200文字以上の場合、回答はなるべく箇条書きを含めてわかりやすく回答してください。\n","context: {context}\n","<|end_of_text|>\n","<|start_of_role|>user<|end_of_role|>question:{question}<|end_of_text|>\n","<|start_of_role|>assistant<|end_of_role|>\n","\"\"\"\n","\n","\n","# Prompt Templateを作成します\n","rag_prompt[key] = PromptTemplate.from_template(template)\n","\n","# LangChainで使うPromptt(RAGなし): granite-3-8b-instruct\n","template = \"\"\"<|start_of_role|>system<|end_of_role|>あなたは2024年にIBMが開発したAI言語モデル「Granite」です。\n","あなたは親切で、礼儀正しく、誠実なアシスタントです。常に安全を保ちながら、できるだけユーザーの役に立つように詳しく回答してください。\n","回答には、有害、非倫理的、⼈種差別的、性差別的、有毒、危険、または違法なコンテンツを含めてはいけません。回答は社会的に偏⾒がなく、本質的に前向きなものであることを確認してください。\n","必ず⽇本語の文章で回答してください。知ったかぶりをしないでください。\n","質問が意味をなさない場合、または事実に⼀貫性がない場合は、正しくないことに答えるのではなく、その理由を説明してください。質問の答えがわからない場合は、誤った情報を共有しないでください。\n","回答が200文字以上の場合、回答はなるべく箇条書きを含めてわかりやすく回答してください。\n","<|end_of_text|>\n","<|start_of_role|>user<|end_of_role|>question:{question}<|end_of_text|>\n","<|start_of_role|>assistant<|end_of_role|>\n","\"\"\"\n","\n","# Prompt Templateを作成します\n","no_rag_prompt[key] = PromptTemplate.from_template(template)\n"]},{"cell_type":"markdown","metadata":{"id":"f2cb9d18-d98a-44c5-9344-970f8a681d5a"},"source":["### 7. Retrieverの作成\n","ここでは「 [3. ベクトル・データベース Milvusとwatsonx.ai LLMでRAGを構成して、質問をしてみよう!](https://github.com/IBM/japan-technology/blob/main/techxchange/2024-watsonx-handson-1/notebooks/techxchange_handson_03_RAG.ipynb)」の **「閾値以上の類似スコアを持つ文書のみを返す」CustomRetriever**を使います.\n","\n","類似スコアの閾値と検索件数をGUIで指定できるようにします。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d3971aa4-5371-41f0-ab1b-92483b184680"},"outputs":[],"source":["# 閾値以上の類似スコアを持つ文書のみを返す」CustomRetrieverを作成\n","# asyncの方は省略\n","from langchain.schema.vectorstore import VectorStoreRetriever\n","from langchain.schema.document import Document\n","from langchain.callbacks.manager import (\n","    CallbackManagerForRetrieverRun,\n",")\n","from typing import List\n","\n","class CustomRetriever(VectorStoreRetriever):\n","    def _get_relevant_documents(\n","        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n","    ) -> List[Document]:\n","        top_k = self.search_kwargs.get(\"k\", 4)\n","        docs_and_similarities = self.vectorstore.similarity_search_with_score(query, k=top_k)\n","                \n","        threshold = self.search_kwargs.get(\"score_threshold\", 0)\n","       \n","        return [doc for doc, score in docs_and_similarities if score > threshold and score < 1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"684dad50-3ef8-4abe-83cc-51913c4d7aa3"},"outputs":[],"source":["# 類似スコアの閾値と検索件数を指定して、Retrieverを作成する関数\n","def get_Retriever(vectorstore, search_score_flag, score, k):\n","    if search_score_flag == \"On\": # 類似スコアの閾値を指定する場合\n","        return CustomRetriever(vectorstore = vectorstore, search_kwargs={\"score_threshold\": score, \"k\":k})\n","    else: # 類似スコアの閾値を指定しない場合\n","        return vectorstore.as_retriever(search_kwargs={\"k\": k})   "]},{"cell_type":"markdown","metadata":{},"source":["### 8. テストしてみます\n","\n","- 作成したRetrieverの類似検索テストしてみます\n","- LLMを指定してChainを作成し、質問してみます"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2916b138-b5be-4858-91c4-856935fedba5"},"outputs":[],"source":["# 作成したRetrieverの類似検索テスト\n","\n","query=\"RAGに関するセッションを教えてください\"\n","\n","retriever = get_Retriever(vector_db, \"On\", 0.83, 20)\n","docs = retriever.invoke(query)\n","# print(docs)\n","for doc in docs:\n","    print({\"content\": doc.page_content, \"metadata\": doc.metadata} )\n","    print(\"---------\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1bfa83a3-6ac2-47d4-bc29-1e8af7089f9c"},"outputs":[],"source":["# Chainの作成\n","# 1つKeyを指定してください\n","\n","# key = 'llama-3-2-3b-instruct'\n","# key = 'mixtral-8x7b-instruct-v01'\n","key = 'granite-3-8b-instruct'\n","retriever = get_Retriever(vector_db, \"On\", 0.83, 10)\n","rag_chain = (\n","    {\"context\": retriever, \"question\": RunnablePassthrough()}\n","    | rag_prompt[key]\n","    | custom_llm[key]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7b56dd8b-c1ca-4776-af44-d4827b371468"},"outputs":[],"source":["# 質問してみます\n","\n","query=\"RAGに関するセッションの詳細を教えてください\"\n","# query=\"watsonx.dataに関するセッションの詳細を教えてください\"\n","# query=\"TechXchangeについて概要を教えてください\"\n","# query=\"量子コンピューター関連のセッションを教えてください\"\n","print(rag_chain.invoke(query))"]},{"cell_type":"markdown","metadata":{"id":"85140aaa-eb55-4185-b0af-c1ba5dc6aabd"},"source":["### 9. gradioでGUI作成\n","\n","[gradio](https://www.gradio.app/)を使ってGUIを作り、いろいろな条件でRAG構成を試してみましょう！\n","\n","尚、会話履歴を読むようにしていないので、毎回フルセンテンスで質問するようにお願いします。\n","\n","プロンプトを変更して試してもOKです。\n","\n"," public URLがインターネット上のURLになります。このLinkは72時間保持されます. \n","\n","\n","share_flag :\n","- Watson Studio仕様の場合はTrue\n","- Trueにするとインターネットに公開されるので、機密情報がある場合はTrueにしないようにしてください。\n","- PC上のnotebookで実行する場合は、FalseにしてLocalPCでの実行が可能です。\n","\n","下のセルを実行するとGUIが表示されます。<br>\n","public URLをクリックして、Webブラウザーで開いて操作してみましょう。<br>\n","エラーメッセージ等はnotebookに出ますので、エラーが発生した場合はnotebookのセル出力のGUIの下の方を参照してください。<br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"66be6220-c5cf-45ba-9727-a71ef9259e25","scrolled":true},"outputs":[],"source":["import gradio as gr\n","from langchain_core.output_parsers import StrOutputParser\n","\n","# Watson Studio仕様の場合はTrue\n","# インターネットに公開されるので、機密情報がある場合はTrueにしないようにしてください。\n","# PC上のnotebookで実行する場合は、FalseにしてLocalPCでの実行が可能です。\n","\n","share_flag = True\n","# share_flag = False\n","\n","def chat_streaming(message, history, RAG_flag, model_name, search_k, search_score_flag, search_score ):\n","    gr_retriever = get_Retriever(vector_db, search_score_flag, search_score, search_k)\n","    gr_rag_chain = None\n","    if RAG_flag == \"Off\":\n","        gr_rag_chain = (\n","            {\"question\": RunnablePassthrough()}\n","            | no_rag_prompt[model_name]\n","            | custom_llm[model_name] \n","        )\n","        print(\"RAG OFF\")\n","    else:\n","        gr_rag_chain = (\n","            {\"context\": gr_retriever, \"question\": RunnablePassthrough()}\n","            | rag_prompt[model_name]\n","            | custom_llm[model_name] \n","        )\n","      \n","    \n","    if message is not None:\n","        partial_message = \"\"\n","        for response in gr_rag_chain.stream(message):\n","            partial_message += response    \n","            yield partial_message \n","\n","def change_search_score(flag):\n","    if flag == \"On\":\n","        interactive_flag = True\n","    else:\n","        interactive_flag = False\n","    return gr.update(interactive = interactive_flag, visible= interactive_flag)    \n","\n","CSS = \"\"\"#row1 {flex-grow: 1; align-items: unset;}\n","    .form {height: fit-content;}\"\"\" \n","\n","with gr.Blocks(fill_height=True,  css=CSS) as demo:\n","    gr.Markdown(\"## TechXchange Japan Q&A\")\n","\n","    with gr.Row(equal_height=False, elem_id=\"row1\"):\n","        with gr.Column(variant=\"panel\", scale=1):\n","            with gr.Accordion(\"設定\"):\n","                RAG_flag = gr.Radio(choices=[\"On\", \"Off\"], type=\"value\", label=\"RAG構成\", value=\"On\", interactive = True)\n","                model_name = gr.Dropdown([\"mixtral-8x7b-instruct-v01\", \"granite-3-8b-instruct\", \"llama-3-2-3b-instruct\"], label=\"LLM Model\", value=\"mixtral-8x7b-instruct-v01\")\n","                search_k = gr.Number(label=\"類似検索の結果取得数(k) 最大15\",minimum=1, maximum=15, value=10, interactive = True )\n","                search_score_flag  = gr.Radio([\"On\", \"Off\"], label=\"類似度スコアの閾値を設定\", value=\"On\", interactive = True)\n","                search_score = gr.Slider(minimum=0, maximum=1, step=0.01, value=0.83, label=\"類似度スコアの閾値\" ,interactive = True, visible=True)\n","            gr.Markdown(\"Graniteは最大トークン数が少ないため,　取得数を上げたり類似度スコアの閾値を下げるとエラーになることがあります。その場合は取得数を少なくしたり類似度スコアの閾値上げて調整してください。\")\n","            \n","        with gr.Column(scale=2):\n","            chatbot = gr.ChatInterface(fn=chat_streaming, type=\"messages\", \n","                                       additional_inputs =[RAG_flag, model_name, search_k, search_score_flag, search_score], \n","                                       title=\"TechXchange Bot\", fill_height=True)\n","          \n","                                    \n","    search_score_flag.change(fn=change_search_score, inputs=search_score_flag, outputs=search_score)\n","  \n","\n","demo.launch(share=share_flag)"]},{"cell_type":"markdown","metadata":{"id":"fc7868d8-466e-42bb-8f0d-396225b333f2"},"source":["### これで「ベクトルDB Milvusとwatsonx.ai LLMでRAGを構成して、チャットアプリを作成してみよう!」は完了です。<br>\n","\n","\n","#### Notebookを保存する場合は、右上の保存アイコンをクリックして保存してください。\n","\n","- <img width=\"400\" alt=\"\" src=\"https://github.com/IBM/japan-technology/blob/main/techxchange/2024-watsonx-handson-1/images/save_notebook.jpg?raw=true\">\n","<br>\n","<br>\n","\n","#### プロジェクトの画面に戻る場合は、右上のプロジェクト名をクリックしてください。\n","\n","- <img width=\"400\" alt=\"\" src=\"https://github.com/IBM/japan-technology/blob/main/techxchange/2024-watsonx-handson-1/images/return_to_project.jpg?raw=true\">\n","<br>\n","<br>\n","\n","#### Notebookを開いたままでプロジェクトの画面を表示するには、上部のプロジェクト名を右クリックし、「新しいタブで開く」でプロジェクトを新しいタブで開いてください。\n","\n","- <img width=\"500\" alt=\"\" src=\"https://github.com/IBM/japan-technology/blob/main/techxchange/2024-watsonx-handson-1/images/open_project_from_notebook.jpg?raw=true\">\n","<br>\n","<br>\n","\n","### ハンズオンはすべて完了しました！　お疲れ様でした！"]}],"metadata":{"kernelspec":{"display_name":"Python 3.11","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":4}
